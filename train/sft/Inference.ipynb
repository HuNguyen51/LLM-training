{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/ai agent trainning/train/sft')"],"metadata":{"id":"sGTwAV_pyOCG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745837408277,"user_tz":-420,"elapsed":1590,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"22ecfed3-ae22-47f3-c352-404ac856cbbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"W56EoL7qyOAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yaml\n","\n","with open('../configs/base.yaml', 'r') as file:\n","    configs = yaml.safe_load(file)\n","\n","with open('../configs/sft.yaml', 'r') as file:\n","    configs.update(yaml.safe_load(file))"],"metadata":{"id":"3jB3UIDcyN-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"taAEvn1bybod"}},{"cell_type":"code","source":["max_new_tokens = 64\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,\\\n","    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng. \\\n","    nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'. \\\n","    h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.\"},\n","\n","    {\"role\": \"user\", \"content\": \"ƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o\"},\n","]\n","\n","model = \"\"\n","unsloth_model, unsloth_tokenizer = None, None\n","peft_model, peft_tokenizer = None, None\n","import gc"],"metadata":{"id":"L-ZDYHr77-9S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unsloth"],"metadata":{"id":"Ohuc14LAzZhg"}},{"cell_type":"code","source":["from transformers import TextStreamer"],"metadata":{"id":"9VbrapuTyzOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if model != \"unsloth\":\n","    print(\"unsloth loading...\")\n","    del peft_model, peft_tokenizer\n","    gc.collect()\n","    model = \"unsloth\"\n","\n","    from unsloth import FastLanguageModel\n","    unsloth_model, unsloth_tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = configs['pretrain_model'], # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = configs['model']['max_seq_length'],\n","        dtype = configs['model']['dtype'],\n","        load_in_4bit = configs['model']['load_in_4bit'],\n","    )\n","    FastLanguageModel.for_inference(unsloth_model) # Enable native 2x faster inference"],"metadata":{"id":"fn6yKCh-ywMF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745837492770,"user_tz":-420,"elapsed":19361,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"5dc76633-7c60-43f4-b525-0db4c4de4056"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["unsloth loading...\n","ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-10-266ad7892883>:7: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.4.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}]},{"cell_type":"code","source":["if model == \"unsloth\":\n","    inputs = unsloth_tokenizer.apply_chat_template(\n","        messages,\n","        tokenize = True,\n","        add_generation_prompt = True, # Must add for generation\n","        return_tensors = \"pt\",\n","    ).to(\"cuda\")\n","\n","    outputs = unsloth_model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True,\n","                            temperature = 1.5, min_p = 0.1)\n","\n","    print(unsloth_tokenizer.batch_decode(outputs))"],"metadata":{"id":"LA-57t1lzEmp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745837499041,"user_tz":-420,"elapsed":6274,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"e868a994-d3de-47c1-edde-667abf3eba02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nB·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng.     nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'.     h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nC·∫£m ∆°n c√¢u h·ªèi c·ªßa b·∫°n. T√¥i hi·ªÉu m·ªëi quan t√¢m c·ªßa b·∫°n. Da ƒë·∫ßu c√≥ th·ªÉ b·ªã ƒëau do r·∫•t nhi·ªÅu nguy√™n nh√¢n kh√°c nhau. V√¨ v·∫≠y, t√¥i khuy√™n b·∫°n n√™n ki·ªÉm tra c√°c y·∫øu t·ªë b√™n ngo√†i. V√≠ d·ª• nh∆∞ lo·∫°i s·ªØa ƒë√°nh rƒÉng c·ªßa b·∫°n l√† g√¨. B·∫°n th∆∞·ªùng s·ª≠ d·ª•ng m√°y\"]\n"]}]},{"cell_type":"code","source":["if model == \"unsloth\":\n","    inputs = unsloth_tokenizer.apply_chat_template(\n","        messages,\n","        tokenize = True,\n","        add_generation_prompt = True, # Must add for generation\n","        return_tensors = \"pt\",\n","    ).to(\"cuda\")\n","\n","    text_streamer = TextStreamer(unsloth_tokenizer, skip_prompt = True)\n","    _ = unsloth_model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = max_new_tokens,\n","                      use_cache = True, temperature = 1.5, min_p = 0.1)"],"metadata":{"id":"9OfVG-EbyaGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745837504488,"user_tz":-420,"elapsed":5445,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"e7eedcf7-784c-4b70-df55-c73d4ed8c0a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["C·∫£m ∆°n c√¢u h·ªèi c·ªßa b·∫°n. ƒêau ƒë·∫ßu l√† nguy√™n nh√¢n do lo l·∫Øng, cƒÉng th·∫≥ng, cƒÉng th·∫≥ng v·ªÅ tinh th·∫ßn v√† c·∫£ v·ªÅ th·ªÉ ch·∫•t. Nguy√™n nh√¢n ƒëau ƒë·∫ßu l√† do nhi·ªÅu t√¨nh hu·ªëng. Trong ƒë√≥:1. ƒêau ƒë·∫ßu n·∫∑ng c√≥ th·ªÉ ƒë∆∞·ª£c g√¢y ra do r·ªëi lo\n"]}]},{"cell_type":"markdown","source":["## Transformers"],"metadata":{"id":"bifYG1Npzee4"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"Tuty6VmDztFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if model != \"peft\":\n","    print(\"peft loading...\")\n","    del unsloth_model, unsloth_tokenizer\n","    gc.collect()\n","    model = \"peft\"\n","\n","    # I highly do NOT suggest - use Unsloth if possible\n","    from peft import AutoPeftModelForCausalLM\n","    from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","    peft_model = AutoPeftModelForCausalLM.from_pretrained(\n","        configs['pretrain_model'], # YOUR MODEL YOU USED FOR TRAINING\n","        load_in_4bit = configs['model']['load_in_4bit'],\n","    )\n","    peft_tokenizer = AutoTokenizer.from_pretrained(configs['pretrain_model'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Uqv9z4a8QNr","executionInfo":{"status":"ok","timestamp":1745837445470,"user_tz":-420,"elapsed":23170,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"8c38a362-909a-44bd-b166-ed42d93ddba7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["peft loading...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n"]}]},{"cell_type":"code","source":["if model == \"peft\": # ƒë√£ ch·∫°y unsloth th√¨ ch·∫°y d√≤ng n√†y s·∫Ω l·ªói\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    peft_model.to(device)\n","    peft_model.eval()\n","\n","    inputs = peft_tokenizer.apply_chat_template(\n","        messages,\n","        tokenize = True,\n","        add_generation_prompt = True, # Must add for generation\n","        return_tensors = \"pt\",\n","    ).to(device)\n","\n","    with torch.no_grad():\n","        outputs = peft_model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True,\n","                          temperature = 1.5, min_p = 0.1)\n","        print(peft_tokenizer.decode(outputs[0]))"],"metadata":{"id":"nh5bz5oAzOQp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745837452815,"user_tz":-420,"elapsed":7343,"user":{"displayName":"Nguyen Hung","userId":"13871841537723911992"}},"outputId":"3556e47a-596a-4269-c575-883a522853ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 26 July 2024\n","\n","B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng.     nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'.     h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","ƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","Xin ch√†o b·∫°n c√≥ v·∫ª ƒëau ƒë·∫ßu. D·∫•u hi·ªáu n√†y th∆∞·ªùng g√¢y s·ªët v√† ƒëau c∆°. Do v·∫≠y t√¥i nghƒ© c√≥ th·ªÉ n√≥ l√† do nhi·ªÖm tr√πng, c·∫£m c√∫m, lyme, ho·∫∑c b·∫•t k·ª≥ b·ªánh t·ª± do n√†o trong t∆∞∆°ng lai. ƒê·ª´ng lo l·∫Øng v·ªÅ ƒëi·ªÅu n√†y. B·∫°n s·∫Ω ·ªïn\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/ai agent trainning/train/sft')"],"metadata":{"id":"sGTwAV_pyOCG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746900076750,"user_tz":-420,"elapsed":1714,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"492d1f13-386c-4b17-d472-8a10ba4f811c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"W56EoL7qyOAc","executionInfo":{"status":"ok","timestamp":1746900087808,"user_tz":-420,"elapsed":11062,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import yaml\n","\n","with open('../configs/base.yaml', 'r') as file:\n","    configs = yaml.safe_load(file)\n","\n","with open('../configs/ppo.yaml', 'r') as file:\n","    configs.update(yaml.safe_load(file))"],"metadata":{"id":"3jB3UIDcyN-g","executionInfo":{"status":"ok","timestamp":1746900087867,"user_tz":-420,"elapsed":58,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"taAEvn1bybod"}},{"cell_type":"code","source":["max_new_tokens = 64\n","\n","messages = [\n","    {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,\\\n","    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng. \\\n","    nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'. \\\n","    h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.\"},\n","\n","    {\"role\": \"user\", \"content\": \"ƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o\"},\n","]\n","\n","model = \"\"\n","unsloth_model, unsloth_tokenizer = None, None\n","peft_model, peft_tokenizer = None, None\n","import gc"],"metadata":{"id":"L-ZDYHr77-9S","executionInfo":{"status":"ok","timestamp":1746900087886,"user_tz":-420,"elapsed":18,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Unsloth"],"metadata":{"id":"Ohuc14LAzZhg"}},{"cell_type":"code","source":["#!IMPORTANT: Trong khi hu·∫•n luy·ªán ppo, khi l∆∞u m√¥ h√¨nh n·∫øu kh√¥ng c√≥ task_type: \"CAUSAL_LM\" th√¨ Unsloth s·∫Ω b·ªã l·ªói"],"metadata":{"id":"Kueu7_7wWthK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TextStreamer"],"metadata":{"id":"9VbrapuTyzOg","executionInfo":{"status":"ok","timestamp":1746899921241,"user_tz":-420,"elapsed":1863,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["if model != \"unsloth\":\n","    print(\"unsloth loading...\")\n","    del peft_model, peft_tokenizer\n","    gc.collect()\n","    model = \"unsloth\"\n","\n","    from unsloth import FastLanguageModel\n","    unsloth_model, unsloth_tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = configs['ppo_pretrain_model'], # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = configs['model']['max_seq_length'],\n","        dtype = configs['model']['dtype'],\n","        load_in_4bit = configs['model']['load_in_4bit'],\n","    )\n","    FastLanguageModel.for_inference(unsloth_model) # Enable native 2x faster inference"],"metadata":{"id":"fn6yKCh-ywMF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746899959744,"user_tz":-420,"elapsed":38493,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"a36c45af-9123-4d21-dcb0-294ed847cd1b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["unsloth loading...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-6-eaa0e646c999>:7: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.4.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}]},{"cell_type":"code","source":["if model == \"unsloth\":\n","    inputs = unsloth_tokenizer.apply_chat_template(\n","        messages,\n","        tokenize = True,\n","        add_generation_prompt = True, # Must add for generation\n","        return_tensors = \"pt\",\n","    ).to(\"cuda\")\n","\n","    outputs = unsloth_model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True,\n","                            temperature = 1.5, min_p = 0.1)\n","\n","    print(unsloth_tokenizer.batch_decode(outputs))"],"metadata":{"id":"LA-57t1lzEmp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746899968031,"user_tz":-420,"elapsed":8285,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"f1921264-0511-4745-8f94-e6662e280410"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nB·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng.     nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'.     h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nƒêau ƒë·∫ßu l√† t√¨nh tr·∫°ng th∆∞·ªùng g·∫∑p, g√¢y kh√≥ ch·ªãu v√† ·∫£nh h∆∞·ªüng ƒë·∫øn ch·∫•t l∆∞·ª£ng cu·ªôc s·ªëng h√†ng ng√†y. C√≥ m·ªôt s·ªë nguy√™n nh√¢n c√≥ th·ªÉ d·∫´n ƒë·∫øn ƒëau ƒë·∫ßu, bao g·ªìm:\\n\\n1. **Th·ªùi gian v√† cƒÉng th·∫≥ng**: C√°c v·∫•n ƒë·ªÅ v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn v√† c∆° th·ªÉ, nh∆∞ cƒÉng\"]\n"]}]},{"cell_type":"code","source":["if model == \"unsloth\":\n","    inputs = unsloth_tokenizer.apply_chat_template(\n","        messages,\n","        tokenize = True,\n","        add_generation_prompt = True, # Must add for generation\n","        return_tensors = \"pt\",\n","    ).to(\"cuda\")\n","\n","    text_streamer = TextStreamer(unsloth_tokenizer, skip_prompt = True)\n","    _ = unsloth_model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = max_new_tokens,\n","                      use_cache = True, temperature = 1.5, min_p = 0.1)"],"metadata":{"id":"9OfVG-EbyaGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746899972974,"user_tz":-420,"elapsed":4941,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"160e6018-ff14-4af5-8d54-9a65c2756c55"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["ƒêau ƒë·∫ßu l√† m·ªôt tri·ªáu ch·ª©ng ph·ªï bi·∫øn v√† c√≥ th·ªÉ do nhi·ªÅu nguy√™n nh√¢n kh√°c nhau. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë nguy√™n nh√¢n g√¢y ƒëau ƒë·∫ßu ph·ªï bi·∫øn:\n","\n","1. **Gi·∫£m blood flow**: Gi·∫£m l∆∞u l∆∞·ª£ng m√°u ƒë·∫øn n√£o c√≥ th·ªÉ g√¢y ƒëau ƒë·∫ßu. ƒêi·ªÅu n√†y c√≥ th·ªÉ do tƒÉng huy·∫øt √°p, r\n"]}]},{"cell_type":"markdown","source":["## Transformers"],"metadata":{"id":"bifYG1Npzee4"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"Tuty6VmDztFp","executionInfo":{"status":"ok","timestamp":1746900104941,"user_tz":-420,"elapsed":5060,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# I highly do NOT suggest - use Unsloth if possible\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","peft_model = AutoPeftModelForCausalLM.from_pretrained(\n","    configs['ppo_pretrain_model'], # YOUR MODEL YOU USED FOR TRAINING\n","    load_in_4bit = configs['model']['load_in_4bit'],\n",")\n","peft_tokenizer = AutoTokenizer.from_pretrained(configs['ppo_pretrain_model'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Uqv9z4a8QNr","executionInfo":{"status":"ok","timestamp":1746900129767,"user_tz":-420,"elapsed":24825,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"e6f133a8-4a63-4861-de9c-e6b611e0d01a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Chuy·ªÉn m√¥ h√¨nh sang GPU n·∫øu c√≥\n","peft_model.to(device)\n","peft_model.eval()\n","\n","inputs = peft_tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(device)\n","\n","# Inference tr√™n m√¥ h√¨nh ch∆∞a h·ª£p nh·∫•t (PEFT)\n","with torch.no_grad():\n","    outputs = peft_model.generate(input_ids = inputs, max_new_tokens = max_new_tokens, use_cache = True,\n","                        temperature = 1.5, min_p = 0.1)\n","    print(peft_tokenizer.decode(outputs[0]))"],"metadata":{"id":"nh5bz5oAzOQp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746900137891,"user_tz":-420,"elapsed":8123,"user":{"displayName":"Hung Nguyen","userId":"01701556196743888520"}},"outputId":"3153e9f3-13f6-44b7-bcc2-24a1a8014fd3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 26 July 2024\n","\n","B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch trong lƒ©nh v·ª±c chƒÉm s√≥c s·ª©c kho·∫ª,    h√£y ph·∫£n h·ªìi t·ª´ng b∆∞·ªõc, ƒë·∫ßu ti√™n l√† xin ch√†o, sau ƒë√≥ l√† ph√¢n t√≠ch v·ªÅ c√¢u h·ªèi, r·ªìi ph·∫£n h·ªìi l·∫°i d·ª±a tr√™n ki·∫øn th·ª©c v·ª´a t√¨m hi·ªÉu, ƒë√°nh gi√° c√¢u h·ªèi c·ªßa user c√≥ li√™n quan ƒë·∫øn s·ª©c kho·∫ª hay kh√¥ng.     nh·ªØng c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kho·∫ª nh∆∞ xe c·ªô, th·ªÉ thao, h√†ng kh√¥ng, v≈© tr·ª•, du l·ªãch,... h√£y tr·∫£ l·ªùi l√† 'kh√¥ng bi·∫øt'.     h√£y suy nghƒ© t·ª´ng b∆∞·ªõc m·ªôt, step by step.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","ƒëau ƒë·∫ßu l√† do nguy√™n nh√¢n n√†o<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","ƒêau ƒë·∫ßu c√≥ th·ªÉ do nhi·ªÅu nguy√™n nh√¢n kh√°c nhau, bao g·ªìm:\n","\n","1. **S∆∞ng tƒ©nh m·∫°ch**: S∆∞ng tƒ©nh m·∫°ch ·ªü ƒë·∫ßu ho·∫∑c c·ªï c√≥ th·ªÉ g√¢y ƒëau v√† kh√≥ ch·ªãu.\n","2. **Ti√™u ch·ª©ng s·ªèi ƒë·∫ßu**: C√°c t√¨nh tr·∫°ng nh∆∞ ti√™u ch·ª©ng s·ªèi ƒë·∫ßu, s\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}